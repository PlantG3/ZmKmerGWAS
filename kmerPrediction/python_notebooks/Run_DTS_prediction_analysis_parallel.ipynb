{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed modules\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, Ridge, LassoCV\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "import random\n",
    "random.seed(12345)\n",
    "\n",
    "import mkl\n",
    "mkl.set_num_threads(1)\n",
    "\n",
    "#required for keras model\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3900)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#explore and/or transform phenotype data\n",
    "def check_normality(numpy_array):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(9,4))\n",
    "    ax[0].hist(numpy_array)\n",
    "    qqplot(numpy_array, line='s', ax=ax[1])\n",
    "    plt.show()\n",
    "    \n",
    "def max_sqrt(x):\n",
    "    return np.sqrt(np.max(x+1) - x)\n",
    "def max_log10(x):\n",
    "    return np.log10(np.max(x+1) - x)\n",
    "def max_inverse(x):\n",
    "    return 1/(np.max(x+1) - x)\n",
    "\n",
    "all_phenos = pd.read_csv(\"../data/maize282_phenotype_summary11oct2021.txt\", sep=\"\\t\", index_col=[0])\n",
    "for pheno in all_phenos.columns:\n",
    "    print(pheno)\n",
    "    sng_pheno = all_phenos[pheno].dropna().copy()\n",
    "    # for oil, transform with boxcox\n",
    "    if pheno == \"Oil\":\n",
    "        sng_pheno = pd.Series(boxcox(sng_pheno)[0], index=sng_pheno.index)\n",
    "        all_phenos[pheno] = sng_pheno\n",
    "    if pheno == \"ULA\":\n",
    "        sng_pheno = pd.Series(boxcox(max_sqrt(np.sqrt(all_phenos[pheno].dropna())))[0], index=sng_pheno.index)\n",
    "        all_phenos[pheno] = sng_pheno\n",
    "    check_normality(all_phenos[pheno].dropna().copy())\n",
    "all_phenos.to_csv(\"../data/maize282_phenotype_transformed11oct2021.txt\", sep=\"\\t\")\n",
    "'''\n",
    "all_phenos = pd.read_csv(\"../data/maize282_phenotype_transformed11oct2021.txt\", sep=\"\\t\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(multiprocessing.cpu_count()) + \" CPUs available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_cpus_vectorizer = 22\n",
    "n_cpus_models = 22\n",
    "print(\"Using \"+str(n_cpus_vectorizer)+ \" CPUs for vectorizer.\")\n",
    "print(\"Using \"+str(n_cpus_models)+ \" CPUs for models.\")\n",
    "\n",
    "#select which models, vectorizors, and testing schemes\n",
    "vectorizors = [\"CountVectorizer\", \"TfidfVectorizer\"]\n",
    "models = [\"LinearRegression\", \"Ridge\", \"Lasso\", \"NeuralNetwork\"]\n",
    "#models = [\"NeuralNetwork\"] # run NN part of models not run on cluster\n",
    "\n",
    "###IN CURRENT SCRIPT splitting can only have one value\n",
    "splitting = ['Cluster_282_11k']\n",
    "#splitting = [\"Random_Kfolds_11k\"]\n",
    "\n",
    "#phenotype = \"Oil\"\n",
    "phenotype = \"ULA\"\n",
    "#phenotype = \"FT\"\n",
    "\n",
    "#dataSets = [\"1M_random_121320_raw_\"+phenotype] #for all analyses with randomly chosen k-mers\n",
    "\n",
    "#dataSets = [\"1M_associated_kmeans_102421_FT\"]\n",
    "#dataSets = [\"1M_associated_randomFolds_102521_FT\"]\n",
    "\n",
    "#dataSets = [\"1M_associated_randomFolds_110121_ULA\"]\n",
    "#dataSets = [\"1M_associated_kmeans_110121_ULA\"]\n",
    "\n",
    "#dataSets = [\"1M_associated_kmeans_110421_Oil\"]\n",
    "#dataSets = [\"1M_associated_randomFolds_110421_Oil\"]\n",
    "\n",
    "include_abandance = [False]\n",
    "#include_abandance = [True] #, False]\n",
    "\n",
    "#specify file name for run results to be save\n",
    "res_folds = datetime.now().strftime('%m%d%Y_%H%M%S')+\"_results_folds.csv\" #results from each fold\n",
    "\n",
    "print(res_folds)\n",
    "\n",
    "#put selections into table of required runs\n",
    "run_params = [] #will contain run perameters from above for all runs to be performed\n",
    "for vec in vectorizors:\n",
    "    for mod in models:\n",
    "        for spl in splitting:\n",
    "            for dat in dataSets:\n",
    "                for inc in include_abandance:\n",
    "                    run_params.append([vec,mod,spl,dat,inc])\n",
    "run_params = pd.DataFrame(run_params, columns=[\"vectorizor\", \"model\", \"splitting\", \"dataSet\", \"include_abandance\"])\n",
    "run_params = run_params.sort_values([\"dataSet\",\"include_abandance\",\"splitting\",\"vectorizor\"]).reset_index(drop=True) #change order for most efficient runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(run, train_set, test_set282, test_setNAM):\n",
    "    #vectorize.\n",
    "    if run[\"vectorizor\"] == \"CountVectorizer\":\n",
    "            #count vectorizer\n",
    "        vectorizer = CountVectorizer()\n",
    "    elif run[\"vectorizor\"] == \"TfidfVectorizer\":\n",
    "        #TFIDF vectorizor\n",
    "        vectorizer = TfidfVectorizer(min_df = 1 , max_df = 1.0, sublinear_tf=True,use_idf=True)\n",
    "\n",
    "        #setup vectorizors based on all sentences in dataset (for genomic prediction context we would have these)\n",
    "    all_sentences = pd.concat([train_set, test_set282, test_setNAM])\n",
    "    vectorizer.fit(all_sentences[\"sentence\"].values)\n",
    "\n",
    "    #create vectorized training and testing sets\n",
    "    X_train = vectorizer.transform(train_set[\"sentence\"].values)\n",
    "    y_train = train_set[\"value\"]\n",
    "\n",
    "    X_test282 = vectorizer.transform(test_set282[\"sentence\"].values)\n",
    "    y_test282 = test_set282[\"value\"]\n",
    "\n",
    "    if len(test_setNAM)>0:\n",
    "        X_testNAM = vectorizer.transform(test_setNAM[\"sentence\"].values)\n",
    "        y_testNAM = test_setNAM[\"value\"]\n",
    "    else:\n",
    "        X_testNAM = pd.DataFrame([])\n",
    "        y_testNAM = pd.Series([])\n",
    "\n",
    "    #print(X_train.shape, len(y_train), X_test282.shape, len(y_test282), X_testNAM.shape, len(y_testNAM))\n",
    "    return X_train, y_train, X_test282, y_test282, X_testNAM, y_testNAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_regression_continuous_data(run, fold, regression, X_train, y_train, setname):\n",
    "    pred = regression.predict(X_train)\n",
    "    obs = y_train.values\n",
    "    res=pd.DataFrame([y_train.index.tolist(),pred,obs], index=[\"Taxa\",\"Pred\",\"Obs\"]).T\n",
    "    for col in run.index:\n",
    "        #print(col, run.loc[col])\n",
    "        res[col]=run.loc[col]\n",
    "    res[\"Fold\"]=fold\n",
    "    res[\"Set\"] = setname\n",
    "    res = res[['Fold', 'Set', 'vectorizor', 'model', 'splitting', 'dataSet', 'include_abandance', 'Taxa', 'Pred', 'Obs']]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval_regression_reps(LR, run, fold, X_train, y_train, X_test282, y_test282, X_testNAM, y_testNAM, reps):\n",
    "    reg_reps=[]\n",
    "    for rep in range(0, reps):\n",
    "        print(rep)\n",
    "        LR.fit(X_train, np.asarray(y_train.values))\n",
    "        #test model on all datasets\n",
    "        res=[]\n",
    "        res.append(test_regression_continuous_data(run, fold, LR, X_train, y_train, setname=\"Train\"))\n",
    "        res.append(test_regression_continuous_data(run, fold, LR, X_test282, y_test282, setname=\"Test282\"))\n",
    "        if len(y_testNAM)>0:\n",
    "            res.append(test_regression_continuous_data(run, fold, LR, X_testNAM, y_testNAM, setname=\"TestNAM\"))\n",
    "        res = pd.concat(res)\n",
    "        res[\"rep\"]=rep\n",
    "        reg_reps.append(res)\n",
    "    reg_reps = pd.concat(reg_reps)\n",
    "    return reg_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval_keras_in_replicate(run, fold, X_train, y_train, X_test282, y_test282, X_testNAM, y_testNAM, reps):\n",
    "    keras_reps=[]\n",
    "    for rep in range(0, reps):\n",
    "        print(rep)\n",
    "        model, history = run_keras_sngl_model(X_train, y_train)\n",
    "        res=[]\n",
    "        res.append(eval_keras_model(run, fold, model, X_train, y_train, setname=\"Train\"))\n",
    "        res.append(eval_keras_model(run, fold, model, X_test282, y_test282, setname=\"Test282\"))\n",
    "        if len(y_testNAM)>0:\n",
    "            res.append(eval_keras_model(run, fold, model, X_testNAM, y_testNAM, setname=\"TestNAM\"))\n",
    "        res = pd.concat(res)\n",
    "        res[\"rep\"]=rep\n",
    "        keras_reps.append(res)\n",
    "    keras_reps = pd.concat(keras_reps)\n",
    "    return keras_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(run, fold, X_train, y_train, X_test282, y_test282, X_testNAM, y_testNAM):\n",
    "    #run analysis\n",
    "    if run[\"model\"] in [\"LinearRegression\", \"Ridge\", \"Lasso\"]:\n",
    "        #Fit regression\n",
    "        if run[\"model\"]==\"LinearRegression\":\n",
    "            LR = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
    "        elif run[\"model\"]==\"Ridge\":\n",
    "            alphas = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]\n",
    "            LR= RidgeCV(alphas, cv=5)\n",
    "        elif run[\"model\"]==\"Lasso\":\n",
    "            LR = LassoCV(cv=5, n_jobs=1, max_iter=2000) #, random_state=12345\n",
    "\n",
    "        res = run_eval_regression_reps(LR, run, fold, X_train, y_train, X_test282, y_test282, X_testNAM, y_testNAM, reps=1)\n",
    "        \n",
    "    elif run[\"model\"]==\"NeuralNetwork\":\n",
    "        #print(\"Neural Net not yet implemented\")\n",
    "        res = run_eval_keras_in_replicate(run, fold, X_train, y_train, X_test282, y_test282, X_testNAM, y_testNAM, reps=10)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_keras_sngl_model(X_train, y_train):\n",
    "    #setup model\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "    model = Sequential()\n",
    "    #model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "    #model.add(layers.Dense(50, input_dim=input_dim, activation='relu'))\n",
    "    model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    #model.summary()\n",
    "\n",
    "    #train model\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    history = model.fit(X_train.toarray(), y_train.values, epochs=200,\n",
    "                        validation_split=0.1,\n",
    "                        batch_size=10, callbacks=[callback], verbose=0)\n",
    "    return model, history\n",
    "\n",
    "def eval_keras_model(run, fold, model, X_train, y_train, setname):\n",
    "    pred = model.predict(X_train.toarray(), batch_size=10, verbose=False).flatten()\n",
    "    obs = y_train.values\n",
    "    res=pd.DataFrame([y_train.index.tolist(), pred,obs], index=[\"Taxa\",\"Pred\",\"Obs\"]).T\n",
    "    for col in run.index:\n",
    "        #print(col, run.loc[col])\n",
    "        res[col]=run.loc[col]\n",
    "    res[\"Fold\"]=fold\n",
    "    res[\"Set\"] = setname\n",
    "    res = res[['Fold', 'Set', 'vectorizor', 'model', 'splitting', 'dataSet', 'include_abandance', 'Taxa','Pred', 'Obs']]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_by_fold(fold, fold_sets, with_abnd, phe_file, abnd_file):\n",
    "    #import set data by fold\n",
    "    sets_in_fold = fold_sets[fold_sets[\"Fold\"]==fold].copy()\n",
    "    train_set_cltvrs = sets_in_fold[sets_in_fold[\"Set\"]==\"282train\"][\"Taxa\"].tolist()\n",
    "    test_set282_cltvrs = sets_in_fold[sets_in_fold[\"Set\"]==\"282test\"][\"Taxa\"].tolist()\n",
    "    print(abnd_file)\n",
    "    \n",
    "    #import abandance and phenotype data\n",
    "    data_set, _, _ = read_data(phe_file,abnd_file,with_abnd)\n",
    "\n",
    "    #create training and testing sets\n",
    "    train_set = data_set.loc[[x for x in train_set_cltvrs if x in data_set.index.tolist()]]\n",
    "    test_set282 = data_set.loc[[x for x in test_set282_cltvrs if x in data_set.index.tolist()]]\n",
    "    #No NAM test set exists for this data\n",
    "    test_setNAM = pd.DataFrame([])\n",
    "    testNAM_uni_tokens = []\n",
    "    print(len(train_set), len(test_set282))\n",
    "\n",
    "    #sainity check\n",
    "    if len([x for x in train_set.index if x in test_set282.index.tolist()]) != 0:\n",
    "        print(\"TRAIN and TEST set CONTAMINATION!!!!!\")\n",
    "        sys.exit(1)\n",
    "    return train_set, test_set282, test_setNAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_by_fold_associated_kmers(fold, fold_sets, phe_file, abnd_train, abnd_test, with_abnd=False):\n",
    "    #import set data by fold\n",
    "    sets_in_fold = fold_sets[fold_sets[\"Fold\"]==fold].copy()\n",
    "    train_set_cltvrs = sets_in_fold[sets_in_fold[\"Set\"]==\"282train\"][\"Taxa\"].tolist()\n",
    "    test_set282_cltvrs = sets_in_fold[sets_in_fold[\"Set\"]==\"282test\"][\"Taxa\"].tolist()\n",
    "    \n",
    "    print(abnd_train)\n",
    "    print(abnd_test)\n",
    "    #import abandance and phenotype data by fold\n",
    "    train_set, train_set_uni_tokens, _ = read_data(phe_file, abnd_file=abnd_train, with_abnd=with_abnd)\n",
    "    train_set = train_set.loc[[x for x in train_set_cltvrs if x in train_set.index.tolist()]]\n",
    "    \n",
    "    test_set282, test282_uni_tokens, _ = read_data(phe_file, abnd_file=abnd_test, with_abnd=with_abnd)\n",
    "    test_set282 = test_set282.loc[[x for x in test_set282_cltvrs if x in test_set282.index.tolist()]]\n",
    "    \n",
    "    test_setNAM = pd.DataFrame([])\n",
    "    testNAM_uni_tokens = []\n",
    "    \n",
    "    #sanity check!\n",
    "    if len([x for x in train_set.index.tolist() if x not in train_set_cltvrs]) != 0:\n",
    "        print(\"STOP!!! Train set does not match cultivar list.\")\n",
    "        sys.exit(1)\n",
    "    if len([x for x in test_set282.index.tolist() if x not in test_set282_cltvrs]) != 0:\n",
    "        print(\"STOP!!! Train set does not match cultivar list.\")\n",
    "        sys.exit(1)\n",
    "    if len([x for x in train_set.index if x in test_set282.index.tolist()]) != 0:\n",
    "        print(\"TRAIN and TEST set CONTAMINATION!!!!!\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print (len(train_set_cltvrs), len(test_set282_cltvrs))\n",
    "    print(len(train_set), len(test_set282))\n",
    "    return train_set, test_set282, test_setNAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring in fold splits\n",
    "if len(run_params[\"splitting\"].unique()) ==1:\n",
    "    if run_params[\"splitting\"].unique()[0] == 'Random_Kfolds_11k':\n",
    "        fold_sets = pd.read_csv(\"../data/random_11k_folds_sets_12Dec2020.csv\")\n",
    "    elif run_params[\"splitting\"].unique()[0] == 'Cluster_282_11k':\n",
    "        fold_sets = pd.read_csv(\"../data/kmeans_11k_folds_sets_12Dec2020.csv\")\n",
    "        \n",
    "include_folds =(0,len(fold_sets[\"Fold\"].unique()))\n",
    "for fold in fold_sets[\"Fold\"].unique()[include_folds[0]:include_folds[1]]:\n",
    "    print(fold, len(fold_sets[(fold_sets[\"Fold\"]==fold) & (fold_sets[\"Set\"]==\"282train\")]),\n",
    "          len(fold_sets[(fold_sets[\"Fold\"]==fold) & (fold_sets[\"Set\"]==\"282test\")]),\n",
    "          len(fold_sets[(fold_sets[\"Fold\"]==fold) & (fold_sets[\"Set\"]==\"NAMtest\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#assigne details for all k-folds vectorizors and sets\n",
    "filt=None\n",
    "fold_vec_runs = []\n",
    "for fold in fold_sets[\"Fold\"].unique()[include_folds[0]:include_folds[1]]:\n",
    "    for vect in run_params[\"vectorizor\"].unique():\n",
    "        for aband in run_params[\"include_abandance\"].unique():\n",
    "            #print(fold)\n",
    "            if run_params[\"dataSet\"][0]==\"1M_associated_kmeans_102421_FT\":\n",
    "                abnd_train = \"../data/11fold_kmean_DTS_102421/train_raw/DTS\"+str(fold)+\"_top1M_kmean_train_raw_KOC.txt\"\n",
    "                abnd_test = \"../data/11fold_kmean_DTS_102421/test_raw/DTS\"+str(fold)+\"_top1M_kmean_test_raw_KOC.txt\"\n",
    "            elif run_params[\"dataSet\"][0]==\"1M_associated_randomFolds_102521_FT\":\n",
    "                abnd_train = \"../data/11_fold_random_DTS_102521/train_raw/DTS\"+str(fold)+\"_top1M_random_train_raw_KOC.txt\"\n",
    "                abnd_test = \"../data/11_fold_random_DTS_102521/test_raw/DTS\"+str(fold)+\"_top1M_random_test_raw_KOC.txt\"\n",
    "            \n",
    "            elif run_params[\"dataSet\"][0]==\"1M_associated_randomFolds_110121_ULA\":\n",
    "                abnd_train = \"../data/11_fold_random_ULA_110121/train_raw/ULA\"+str(fold)+\"_top1M_random_train_raw_KOC.txt\"\n",
    "                abnd_test = \"../data/11_fold_random_ULA_110121/test_raw/ULA\"+str(fold)+\"_top1M_random_test_raw_KOC.txt\"\n",
    "            elif run_params[\"dataSet\"][0]==\"1M_associated_kmeans_110121_ULA\":\n",
    "                abnd_train = \"../data/11_fold_kmean_ULA_110121/train_raw/ULA\"+str(fold)+\"_top1M_kmean_train_raw_KOC.txt\"\n",
    "                abnd_test = \"../data/11_fold_kmean_ULA_110121/test_raw/ULA\"+str(fold)+\"_top1M_kmean_test_raw_KOC.txt\"\n",
    "            \n",
    "            elif run_params[\"dataSet\"][0]==\"1M_associated_kmeans_110421_Oil\":\n",
    "                abnd_train = \"../data/11_fold_kmean_oil_110421/train_raw/oil\"+str(fold)+\"_top1M_kmean_train_raw_KOC.txt\"\n",
    "                abnd_test = \"../data/11_fold_kmean_oil_110421/test_raw/oil\"+str(fold)+\"_top1M_kmean_test_raw_KOC.txt\"\n",
    "            elif run_params[\"dataSet\"][0]==\"1M_associated_randomFolds_110421_Oil\":\n",
    "                abnd_train = \"../data/11_fold_random_oil_110421/train_raw/oil\"+str(fold)+\"_top1M_random_train_raw_KOC.txt\"\n",
    "                abnd_test = \"../data/11_fold_random_oil_110421/test_raw/oil\"+str(fold)+\"_top1M_random_test_raw_KOC.txt\"\n",
    "\n",
    "            elif run_params[\"dataSet\"][0]==\"1M_random_121320_raw_\"+phenotype:\n",
    "                abnd_train=\"\"\n",
    "                abnd_test=\"\"\n",
    "                #set k-mer abandance file\n",
    "                abnd_file=\"../data/maize282.k31.random.1M.KOC.txt\"\n",
    "            fold_vec_runs.append([fold, vect, run_params[\"splitting\"].iloc[0], run_params[\"dataSet\"].iloc[0],\n",
    "                                  aband, abnd_train, abnd_test])\n",
    "fold_vec_runs = pd.DataFrame(fold_vec_runs, columns = [\"Fold\", \"vectorizor\", \"splitting\",\"dataSet\",\"include_abandance\", \"abnd_train\", \"abnd_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(fold_vec_runs)\n",
    "print(fold_vec_runs[\"abnd_train\"].str.split(\"/\", expand=True))\n",
    "print(fold_vec_runs[\"abnd_test\"].str.split(\"/\", expand=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(phe_file, abnd_file, with_abnd):\n",
    "    #read in data\n",
    "    if type(phe_file) == pd.core.frame.DataFrame:\n",
    "        kmer_phe = phe_file.copy()\n",
    "    else:\n",
    "        kmer_phe = pd.read_csv(phe_file, sep=\"\\t\", index_col=[0]).dropna()\n",
    "    kmer_abnd = pd.read_csv(abnd_file, sep=\"\\t\", index_col=[0])\n",
    "    \n",
    "    if type(filt)==int:\n",
    "        pval_col = [x for x in kmer_abnd.columns if x.split(\"_\")[-1]==\"pvals\"]\n",
    "        if len(pval_col)==1:\n",
    "            kmer_abnd = kmer_abnd.sort_values(pval_col[0]) # sort by p-value\n",
    "            kmer_abnd = kmer_abnd.iloc[:filt] #take only the top \"filt\" number\n",
    "            print(\"Selecting the top \"+str(filt)+\"k-mers based on \"+pval_col[0])\n",
    "            print(\"Max and min pvals:\", kmer_abnd[pval_col[0]].max(), kmer_abnd[pval_col[0]].min())\n",
    "            print(\"dataset size:\", len(kmer_abnd))\n",
    "        else:\n",
    "            print(\"Randomly selecting \"+str(filt)+\" kmers.\")\n",
    "            rand_sample = random.sample(kmer_abnd.index.tolist(), filt)\n",
    "            kmer_abnd = kmer_abnd.loc[rand_sample]\n",
    "    \n",
    "    #fix column name issues\n",
    "    kmer_abnd.index.name = \"id\"\n",
    "    kmer_abnd.rename(columns={\"B73-1\":\"B73\", \"B97-1\":\"B97\"}, inplace=True)\n",
    "    kmer_phe.index.name = \"Taxa\"\n",
    "    kmer_phe.columns = [\"phe\"]\n",
    "    kmer_phe = kmer_phe.loc[[x for x in kmer_phe.index.tolist() if x in kmer_abnd.columns.tolist()]]\n",
    "    \n",
    "    #Create token list\n",
    "    #with_abnd=True #include multiple copies of each kmer based on its abandance\n",
    "    token_list=[]\n",
    "    for geno in [x for x in kmer_phe.index.tolist() if x in kmer_abnd.columns.tolist()]:\n",
    "        if with_abnd:\n",
    "            tmp = kmer_abnd[kmer_abnd[geno]>0][geno].reset_index()\n",
    "            #tmp = ((tmp[\"id\"].str.lower()+\" \") * tmp[geno]).str.strip()#.str.split(\" \")\n",
    "            tmp = ((tmp[\"id\"].str.lower()+\" \") * tmp[geno].round().astype(int)).str.strip()#.str.split(\" \")\n",
    "        else:\n",
    "            tmp = kmer_abnd[kmer_abnd[geno]>0].index.str.lower().tolist()\n",
    "        token_list.append(\" \".join(tmp))\n",
    "    kmer_phe[\"tokens\"]=token_list\n",
    "    unique_tokens = list(set(kmer_abnd.index.tolist()))\n",
    "    kmer_phe.columns = [\"value\",\"sentence\"]\n",
    "    return kmer_phe, unique_tokens, kmer_abnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_save_vec_sets(fold_vec_run, base):\n",
    "    start = time.time() - base\n",
    "    out_file = \"@\".join(fold_vec_run[[\"Fold\",\"vectorizor\",\"splitting\",\"dataSet\",\"include_abandance\"]].astype(str).tolist())+\".p\"\n",
    "    #print(out_file)\n",
    "    if out_file in os.listdir(\"../data/TMP_vectorizors/\"):\n",
    "        print(\"File already exsist: \"+ out_file)\n",
    "    else:\n",
    "        print(\"Loading Dataset and creating file: \"+ out_file)\n",
    "        if (run_params[\"dataSet\"][0]==\"1M_random_121320_raw_\"+phenotype):\n",
    "            train_set, test_set282, test_setNAM = import_data_by_fold(fold_vec_run[\"Fold\"], fold_sets.copy(), with_abnd=fold_vec_run[\"include_abandance\"],\n",
    "                                                                      phe_file = all_phenos[[run_params[\"dataSet\"][0].split(\"_\")[-1]]].dropna(),\n",
    "                                                                      abnd_file = abnd_file)\n",
    "            print(abnd_file)\n",
    "        else:\n",
    "            train_set, test_set282, test_setNAM = import_data_by_fold_associated_kmers(fold_vec_run[\"Fold\"], fold_sets.copy(), with_abnd=fold_vec_run[\"include_abandance\"],\n",
    "                                                                                        phe_file = all_phenos[[run_params[\"dataSet\"][0].split(\"_\")[-1]]].dropna(),\n",
    "                                                                                        abnd_train = fold_vec_run[\"abnd_train\"],\n",
    "                                                                                        abnd_test = fold_vec_run[\"abnd_test\"])\n",
    "            print(fold_vec_run[\"abnd_train\"])\n",
    "            print(fold_vec_run[\"abnd_test\"])\n",
    "        print(len(train_set), len(test_set282), len(test_setNAM))\n",
    "\n",
    "        print(\"Creating new vectorized training and testing sets. \"+ out_file)\n",
    "        #vectorize data sets\n",
    "        X_train, y_train, X_test282, y_test282, X_testNAM, y_testNAM = vectorize(fold_vec_run, train_set, test_set282, test_setNAM)\n",
    "        print(X_train.shape, len(y_train), X_test282.shape, len(y_test282), X_testNAM.shape, len(y_testNAM), out_file)\n",
    "\n",
    "        print(\"saving pickle \"+ out_file)\n",
    "\n",
    "        with open(\"../data/TMP_vectorizors/\"+out_file, \"wb\") as out_p:\n",
    "            pickle.dump([X_train, y_train, X_test282, y_test282, X_testNAM, y_testNAM], out_p)\n",
    "\n",
    "    stop = time.time() - base\n",
    "    return start, stop, out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def multiprocess(func, args, workers):\n",
    "    begin_time = time.time()\n",
    "    with ProcessPoolExecutor(max_workers=workers) as executor:\n",
    "        res = executor.map(func, args, [begin_time for i in range(len(args))])\n",
    "    return list(res)\n",
    "\n",
    "#run all in parallel and save results\n",
    "args = [fold_vec_runs.loc[x] for x in fold_vec_runs.index]\n",
    "results = multiprocess(create_save_vec_sets, args, n_cpus_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run each fold_vec_run on it own cpu\n",
    "def run_save_analysis_sets(fold_vec_run, base):\n",
    "    start = time.time() - base\n",
    "    \n",
    "    ### IMPORT SETS ###\n",
    "    print(\"Importing new vectorized training and testing sets.\")\n",
    "    #print(fold_vec_run)\n",
    "    in_file = \"@\".join(fold_vec_run[['Fold','vectorizor', 'splitting', 'dataSet', 'include_abandance']].astype(str).tolist())+\".p\"\n",
    "    print(in_file)\n",
    "    with open(\"../data/TMP_vectorizors/\"+in_file, \"rb\") as f:\n",
    "        X_train, y_train, X_test282, y_test282, X_testNAM, y_testNAM = pickle.load(f)\n",
    "    #X_train = X_train[:,:100] #for testing\n",
    "    #X_test282 = X_test282[:,:100]\n",
    "    print(X_train.shape, len(y_train), X_test282.shape, len(y_test282), X_testNAM.shape, len(y_testNAM))\n",
    "    \n",
    "    ### Run each analysis ###\n",
    "    print(\"Running Analysis.\")\n",
    "    #setup run params\n",
    "    run_params_tmp = run_params[(run_params[\"vectorizor\"]==fold_vec_run[\"vectorizor\"]) &\n",
    "                                (run_params[\"splitting\"]==fold_vec_run[\"splitting\"]) &\n",
    "                                (run_params[\"dataSet\"]==fold_vec_run[\"dataSet\"]) &\n",
    "                                (run_params[\"include_abandance\"]==fold_vec_run[\"include_abandance\"])].copy()\n",
    "    for run in run_params_tmp.iterrows():\n",
    "        run = run[1]\n",
    "        if run[\"model\"] == \"NeuralNetwork\":\n",
    "            print(\"NeuralNetwork cannot be run in parallel. Will be run in series as end.\")\n",
    "            continue\n",
    "        print(run[\"model\"])\n",
    "        run_results = run_analysis(run, fold_vec_run[\"Fold\"], X_train, y_train, X_test282, y_test282, X_testNAM, y_testNAM)\n",
    "        run_results[\"refreshed\"]=\"vectorizor\"\n",
    "    \n",
    "        #record and save results\n",
    "        #check if results file exist\n",
    "        if res_folds not in os.listdir(\"../results\"):\n",
    "            run_results.to_csv(\"../results/\"+res_folds, mode=\"w\")\n",
    "        else:\n",
    "            run_results.to_csv(\"../results/\"+res_folds, mode=\"a\")\n",
    "\n",
    "    stop = time.time() - base\n",
    "    return start, stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len([x for x in run_params[\"model\"].unique() if x != \"NeuralNetwork\"]) > 1:\n",
    "    print(\"running non-neural network models in parallel.\")\n",
    "    results = multiprocess(run_save_analysis_sets, args, n_cpus_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if \"NeuralNetwork\" in run_params[\"model\"].unique():\n",
    "    print(\"Running Neural Network models in series.\")\n",
    "    run_params_NN = run_params[run_params[\"model\"]==\"NeuralNetwork\"].copy().reset_index(drop=True)\n",
    "    print(run_params_NN)\n",
    "    \n",
    "    results_folds=[]\n",
    "    for fold in fold_sets[\"Fold\"].unique()[include_folds[0]:include_folds[1]]:\n",
    "        print(fold)\n",
    "        results_folds=[] #record which data was refreshed\n",
    "        prev_run=pd.Series(index=run_params_NN.loc[0].index, dtype='str')\n",
    "        for run in run_params_NN.iterrows():\n",
    "            refresh=[] #record which data was refreshed\n",
    "            run=run[1]\n",
    "            print(run.tolist())\n",
    "\n",
    "            #import data, and vectorizor\n",
    "            #vectorize training and testing sets\n",
    "            if ((run[\"dataSet\"]== prev_run[\"dataSet\"]) and\n",
    "                (run[\"include_abandance\"]==prev_run[\"include_abandance\"]) and\n",
    "                (run[\"splitting\"]== prev_run[\"splitting\"]) and \n",
    "                (run[\"vectorizor\"]== prev_run[\"vectorizor\"])):\n",
    "                print(\"Using previous vectorizor.\")\n",
    "            else:\n",
    "                print(\"Importing new vectorized training and testing sets.\")\n",
    "                in_file = \"@\".join([str(fold)]+run[['vectorizor', 'splitting', 'dataSet', 'include_abandance']].astype(str).tolist())+\".p\"\n",
    "                with open(\"../data/TMP_vectorizors/\"+in_file, \"rb\") as f:\n",
    "                    X_train, y_train, X_test282, y_test282, X_testNAM, y_testNAM = pickle.load(f)\n",
    "                print(X_train.shape, len(y_train), X_test282.shape, len(y_test282), X_testNAM.shape, len(y_testNAM))\n",
    "                refresh.append(\"vectorizor\")\n",
    "\n",
    "            #run the analysis\n",
    "            print(\"Running Analysis.\")\n",
    "            run_results = run_analysis(run, fold, X_train, y_train, X_test282, y_test282, X_testNAM, y_testNAM)\n",
    "            run_results[\"refreshed\"]=str(refresh)\n",
    "\n",
    "            #save data\n",
    "            #record and save results\n",
    "            #check if results file exist\n",
    "            if res_folds not in os.listdir(\"../results\"):\n",
    "                run_results.to_csv(\"../results/\"+res_folds, mode=\"w\")\n",
    "            else:\n",
    "                run_results.to_csv(\"../results/\"+res_folds, mode=\"a\")\n",
    "            results_folds.append(run_results)\n",
    "\n",
    "            prev_run = run.copy()\n",
    "    results_folds = pd.concat(results_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
